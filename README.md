# 🧠 Ollama Local Chatbot  
## 🎯 Run LLMs Locally with Python Terminal Interface 🚀

---

## 👋 Introduction

A lightweight, terminal-based chatbot built in Python that connects directly with locally hosted LLMs using the [Ollama](https://ollama.com) API. No internet required after model download — perfect for offline experiments and fast prototyping.

---

## 🌐 Live Demo / Repository Links

> 💻 No live demo – this is a local project  
> 📦 [GitHub Repo](https://github.com/DishantBhere/local-chatbot)

---

## 📸 Screenshots

⚠️ Note: Apologies for the image quality. Due to hardware limitations on my personal system, I was unable to run the model locally. The screenshots were taken using a college lab machine where the setup was successful.

![Screenshot 1](https://github.com/user-attachments/assets/005275ed-f93d-4a04-9101-3d24a49d2fe2) ![Screenshot 2](https://github.com/user-attachments/assets/14d08e2b-2e76-4864-a2bf-ec1da0cc3e4a) ![Screenshot 3](https://github.com/user-attachments/assets/c1c90c0b-1fb3-4957-bd96-ce9f9e2d3564)










---

## 📌 Overview

This project allows you to run LLMs like `llama3`, `mistral`, or `gemma` on your machine through a simple Python script. It sends user input to Ollama's local server and returns model responses in the terminal.

---

## 🌟 Features

| Feature         | Description                                        |
|------------------|----------------------------------------------------|
| 🧠 Local AI       | No cloud required – runs on your machine          |
| 💬 Simple UI      | Just run from terminal, no fancy frontend         |
| ⚡ Fast Startup   | Lightweight and quick to load                     |
| 🔄 Extensible     | Easy to modify or extend to other UIs or tools    |

---

## 🧰 Tech Stack

- 🐍 Python 3.x  
- 🧠 Ollama (Local LLM runtime)  
- 🔗 `requests` Python library  

---

## 📁 Folder Structure
```
ollama-local-chatbot/
├── main.py # Python script to interact with Ollama
├── README.md # Project documentation
└── requirements.txt # (Optional) list of dependencies
```

---

## 🚀 Getting Started

### 1. Install Ollama & Pull a Model

Install [Ollama](https://ollama.com) for your OS, then pull a model like:

```bash
ollama pull llama3
```
2. Clone the Repository
```
git clone https://github.com/yourusername/ollama-local-chatbot.git
cd ollama-local-chatbot

```
3. Run the Chatbot
```
python main.py
```
---

# 🧠  What I Learned
Using Ollama to run open-source LLMs locally

Building a minimal chat interface using Python

Handling local APIs and JSON responses

---

# 🚀 Future Improvements
 Add a basic web interface using Flask

 Multi-model switching

 Add markdown/text formatting to responses

# 🙋‍♂️ Built By

👨‍💻 Dishant Bhere

📧 dishantwork7@gmail.com

🌐 GitHub Profile

📍 India


---
