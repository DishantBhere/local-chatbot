# 🧠 Ollama Local Chatbot  
## 🎯 Run LLMs Locally with Python Terminal Interface 🚀

---

## 👋 Introduction

A lightweight, terminal-based chatbot built in Python that connects directly with locally hosted LLMs using the [Ollama](https://ollama.com) API. No internet required after model download — perfect for offline experiments and fast prototyping.

---

## 🌐 Live Demo / Repository Links

> 💻 No live demo – this is a local project  
> 📦 [GitHub Repo](https://github.com/DishantBhere/local-chatbot)

---

## 📸 Screenshots






![Screenshot_20250730_160710](https://github.com/user-attachments/assets/e8b79e65-fe8d-453d-b62e-86f8949d2aed)
![Screenshot_20250730_160724](https://github.com/user-attachments/assets/3737dc6b-9a31-4dd8-bf73-a0f0581c2af2)
![Screenshot_20250730_160724](https://github.com/user-attachments/assets/8b392fbe-40dc-4bfb-8d63-08bf1778d8c2)

---

## 📌 Overview

This project allows you to run LLMs like `llama3`, `mistral`, or `gemma` on your machine through a simple Python script. It sends user input to Ollama's local server and returns model responses in the terminal.

---

## 🌟 Features

| Feature         | Description                                        |
|------------------|----------------------------------------------------|
| 🧠 Local AI       | No cloud required – runs on your machine          |
| 💬 Simple UI      | Just run from terminal, no fancy frontend         |
| ⚡ Fast Startup   | Lightweight and quick to load                     |
| 🔄 Extensible     | Easy to modify or extend to other UIs or tools    |

---

## 🧰 Tech Stack

- 🐍 Python 3.x  
- 🧠 Ollama (Local LLM runtime)  
- 🔗 `requests` Python library  

---

## 📁 Folder Structure
```
ollama-local-chatbot/
├── main.py # Python script to interact with Ollama
├── README.md # Project documentation
└── requirements.txt # (Optional) list of dependencies
```

---

## 🚀 Getting Started

### 1. Install Ollama & Pull a Model

Install [Ollama](https://ollama.com) for your OS, then pull a model like:

```bash
ollama pull llama3
```
2. Clone the Repository
```
git clone https://github.com/yourusername/ollama-local-chatbot.git
cd ollama-local-chatbot

```
3. Run the Chatbot
```
python main.py
```
---

# 🧠  What I Learned
Using Ollama to run open-source LLMs locally

Building a minimal chat interface using Python

Handling local APIs and JSON responses

---

# 🚀 Future Improvements
 Add a basic web interface using Flask

 Multi-model switching

 Add markdown/text formatting to responses

# 🙋‍♂️ Built By

👨‍💻 Dishant Bhere

📧 dishantwork7@gmail.com

🌐 GitHub Profile

📍 India


---
